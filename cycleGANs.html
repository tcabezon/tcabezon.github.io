<!DOCTYPE html>
    <head>
        <title>Tomascbzn</title>
        <meta name="viewport" content="width=device-width,initial-scale=1">
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
        <link href="CSSTemplate.css" type="text/css" rel="stylesheet" />
    
    </head>
    <body>
        <div id="page">
            <!-- HEADER -->
            <script language="javascript" type="text/javascript" src="js/header.js"></script>
            <script>
                document.querySelectorAll("a[href='projects.html']")[0].className="underline";
                document.querySelectorAll("a[href='CycleGANs.html']")[0].className="underline";
            </script>
            <!-- end HEADER -->
            <div class=content>
                <h1 class=smaller>Capabilities and limitations of style transfer with CycleGANs for ring design automatic generation</h1>
                <br>
                <p>
                    WHEN?  Spring 2021   <br>WHO? Tomás Cabezón    <br>WHY? Master Thesis<br>WHERE?  UOC</p>
                <br>
                <div class='container  justify-content-left'>
                    <img src='images/cycleGANs/cycleGANs1.gif' class=' mx-auto d-block img-fluid  col-sm-10 '>
                </div>
                <p>
                    Rendering programs have changed the design process completely as they permit to see how the products will look like before they are fabricated. However, the rendering process is complicated and takes a lot of time not only in the rendering itself but in the setting of the scene as well. Materials, lights and cameras need to be set in order to get the best quality results, nevertheless, the optimal output may not be obtained in the first render. This all makes the rendering process a tedious process.
                    <br><br>
                    Since Zhu et al. introduced Generative Adversarial Networks (GANs) in 2014, they have been used to obtain computer-generated data. From non-existing human faces to medical data analysis or image style transfer. GANs have been used to transfer image textures from one domain to another, but paired data was needed. When this same group introduced the CycleGANs, this all changed. CycleGANs allow transforming one image from one domain to another, without the need of paired data.
                    <br><br>
                    This Work studies the possibilities of CycleGANs on style transfer from an initial sketch to a final render. A process that is crucial in the automatic generation of ring designs as allows the costumer to see the final products before buying.
                    <br><br>
                    The present Work sets a basis for future research, showing the possibilities of GANs in design and establishing a starting point for new applications. 
                </p>
                <br><br>
                <h3>Results</h3>
                <br>
                <div class='container-fluid'>
                    <br>
                    <img src='images/cycleGANs/cycleGANs2.jpeg' class=' mx-auto d-block img-fluid  col-sm-10 '>
                    <br>
                    <p>An example of how this CycleGAN could be implemented is bellow</p>
                    <br>
                    <img src='images/cycleGANs/cycleGANs3.gif' class=' mx-auto d-block img-fluid  col-sm-10 '>
                </div>
                <h3>Usage</h3>
                <br>
                <p>The code to train and test the CycleGAN can be found in the following <a class=underline href="https://github.com/tcabezon/automatic-ring-design-generation-cycleGAN" target="_blank">github repository</a>. If you want to train a CycleGAN from scratch you can upload your database and use the train code. If you want to use and already trained model, you can use the test code.</p>
                <br><br>
                <h3>Academic Project</h3>
                <br>
                <p>Final Master Publication can be found <a class=underline href="http://hdl.handle.net/10609/136047" target="_blank">here</a>.</p>
                
                <br><br><br><br><p class=footer>©Tomas Cabezon Pedroso, 2021</p>

            </div>
        </div>
    </body>